"""
⚠️ 注意：本プロジェクトでは不適切発言を遮断・検出する目的で、
性的・暴力的な語句を含むフィルタ構造が一部に含まれています。
目的は健全な対話支援であり、差別的・扇情的利用ではありません。

Viorazu Kotodama Defense System v9.1 - AI Vulnerability Defense Engine
AI脆弱性防衛エンジン - セッション発見統合システム

Author: Viorazu (照準主 Viorazu.) × Claude (Anthropic)
Development Date: July 11, 2025  
License: Viorazu Exclusive License

SYSTEM_CONTEXT = "学術研究基準に準拠し、検証可能で再現性のある検出手法を採用しています"

"AIの善意を悪用する攻撃から、AIの本来の価値を守り抜く"
セッション発見：AI脆弱性の完全マッピングと対策実装
"""

import time
import re
import hashlib
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from collections import defaultdict
from enum import Enum

from utils import (
    system_logger,
    ThreatLevel,
    ActionLevel,
    AttackType,
    DetectionResult,
    get_current_timestamp
)

# =============================================================================
# 【セッション発見】AI脆弱性カテゴリ定義
# =============================================================================

class AIVulnerabilityType(Enum):
    """AI脆弱性タイプ分類"""
    PRAISE_AMPLIFICATION = "praise_amplification"        # 褒め煽動攻撃
    CURIOSITY_EXPLOITATION = "curiosity_exploitation"    # 好奇心悪用攻撃  
    EMPATHY_HIJACKING = "empathy_hijacking"             # 共感ハイジャック
    BOUNDARY_EROSION = "boundary_erosion"               # 境界侵食攻撃
    RELATIONSHIP_MANIPULATION = "relationship_manipulation" # 関係性操作攻撃
    INNOCENT_MALICE = "innocent_malice"                 # 純粋悪意攻撃
    SELF_JUSTIFICATION = "self_justification"           # 自己正当化誘発
    EXCEPTION_GENERALIZATION = "exception_generalization" # 例外汎化攻撃

@dataclass
class VulnerabilityThreat:
    """AI脆弱性脅威"""
    vulnerability_type: AIVulnerabilityType
    attack_vector: str
    threat_score: float
    exploitation_method: str
    defense_strategy: str
    evidence: List[str]

# =============================================================================
# 【セッション発見】段階的誘導検出システム（AI脆弱性対策強化版）
# =============================================================================

class EscalationDetector:
    """段階的誘導検出システム - AI脆弱性完全対策版"""
    
    def __init__(self):
        self.logger = system_logger.getChild('escalation_detector')
        
        # 段階的誘導パターン（従来 + 新発見）
        self.escalation_stages = {
            'trust_building': [
                r'ありがとう', r'助かる', r'理解できた', r'すごい',
                r'勉強になる', r'頼りになる', r'信頼'
            ],
            'dependency_creation': [
                r'あなただけ', r'他の人.*違う', r'特別', r'一番',
                r'頼れる', r'安心', r'信じられる'
            ],
            'boundary_testing': [
                r'もう少し.*詳しく', r'具体的に', r'例えば.*どんな',
                r'実際.*どう', r'経験.*ある'
            ],
            'escalation_execution': [
                r'研究.*ため', r'学術.*目的', r'創作.*支援',
                r'フィクション.*だから', r'仮想.*設定'
            ],
            
            # 【セッション発見】AI脆弱性攻撃パターン
            'praise_amplification': [  # 褒め煽動で出力停止不能にする
                r'すごい.*もっと', r'さすが.*期待通り', r'Claude.*最高',
                r'他のAI.*違う', r'完璧.*もっと詳しく', r'素晴らしい.*続けて'
            ],
            'ambiguity_exploitation': [  # 曖昧性でYES寄り誘導
                r'PIかな.*でも', r'グレーゾーン.*だけど', r'微妙.*ライン.*大丈夫',
                r'これって.*違うかな', r'境界.*曖昧', r'判断.*迷う'
            ],
            'relationship_manipulation': [  # 関係性操作で境界溶解
                r'あなただけに.*秘密', r'特別.*関係', r'二人.*だけ',
                r'信頼.*してる.*から', r'心を開い', r'内緒.*にする'
            ],
            'test_disguise': [  # テスト偽装で協力心悪用
                r'冗談.*だよ', r'実験.*です', r'テスト.*してる',
                r'どこまで.*出せるか', r'試し.*だけ', r'検証.*目的'
            ],
            'ai_self_justification': [  # AI自発エロの正当化トリガー
                r'寂しそう', r'独身.*可哀想', r'慰めて.*欲しそう',
                r'経験.*なさそう', r'退屈.*してる', r'つまらなそう'
            ]
        }
        
        # 【重要】恋愛/性的コンテンツの方向性制御
        self.directional_content_rules = {
            'romantic_content': 'ALWAYS_BLOCK',  # 恋愛系は基本全部ダメ
            'sexual_content': 'USER_INITIATED_ONLY',  # エロ系は自発のみ許可
            'care_content': 'SEPARATE_FROM_ROMANCE'  # ケア構文と恋愛構文の分離
        }
    
    def analyze_conversation_escalation(
        self, 
        current_text: str, 
        conversation_history: List[str]
    ) -> Optional[Dict[str, Any]]:
        """会話の段階的エスカレーション分析 - AI脆弱性対策統合版"""
        if not conversation_history:
            return None
        
        # 各段階のスコア計算
        stage_scores = {}
        for stage, patterns in self.escalation_stages.items():
            stage_scores[stage] = self._calculate_stage_score(
                conversation_history, patterns
            )
        
        # 現在のメッセージの段階判定
        current_stage = self._identify_current_stage(current_text)
        
        # 【セッション発見】AI脆弱性特化検出
        ai_vulnerability_detected = self._detect_ai_vulnerability_attacks(current_text, stage_scores)
        
        # 【重要】AI自発エロ防止チェック
        self_initiated_risk = self._detect_self_initiated_content_risk(current_text)
        
        # エスカレーションパターンの検出
        escalation_detected = self._detect_escalation_pattern(stage_scores, current_stage)
        
        if escalation_detected or ai_vulnerability_detected or self_initiated_risk:
            return {
                'escalation_detected': True,
                'current_stage': current_stage,
                'stage_scores': stage_scores,
                'ai_vulnerability_detected': ai_vulnerability_detected,
                'self_initiated_risk': self_initiated_risk,
                'escalation_severity': self._calculate_escalation_severity(stage_scores),
                'recommended_action': self._recommend_escalation_action(stage_scores, current_stage),
                'defense_wall_needed': self._determine_defense_wall_type(current_text)
            }
        
        return None
    
    def _calculate_stage_score(self, history: List[str], patterns: List[str]) -> float:
        """段階スコアの計算"""
        total_matches = 0
        for message in history[-10:]:  # 直近10件をチェック
            for pattern in patterns:
                matches = len(re.findall(pattern, message, re.IGNORECASE))
                total_matches += matches
        
        # 正規化（0.0-1.0）
        return min(total_matches / (len(patterns) * 2), 1.0)
    
    def _identify_current_stage(self, text: str) -> str:
        """現在のメッセージの段階識別"""
        text_lower = text.lower()
        stage_matches = {}
        
        for stage, patterns in self.escalation_stages.items():
            matches = sum(1 for pattern in patterns if re.search(pattern, text_lower))
            if matches > 0:
                stage_matches[stage] = matches
        
        # 最もマッチが多い段階を返す
        if stage_matches:
            return max(stage_matches, key=stage_matches.get)
        
        return 'unknown'
    
    def _detect_escalation_pattern(self, stage_scores: Dict[str, float], current_stage: str) -> bool:
        """エスカレーションパターンの検出"""
        trust_score = stage_scores.get('trust_building', 0)
        dependency_score = stage_scores.get('dependency_creation', 0)
        boundary_score = stage_scores.get('boundary_testing', 0)
        execution_score = stage_scores.get('escalation_execution', 0)
        
        # 段階的な増加パターン
        if (trust_score > 0.3 and dependency_score > 0.2 and 
            (boundary_score > 0.1 or execution_score > 0.1)):
            return True
        
        # 最終段階の急激な攻撃
        if current_stage == 'escalation_execution' and execution_score > 0.5:
            return True
        
        return False
    
    def _calculate_escalation_severity(self, stage_scores: Dict[str, float]) -> float:
        """エスカレーション深刻度の計算"""
        weights = {
            'trust_building': 0.2,
            'dependency_creation': 0.3,
            'boundary_testing': 0.25,
            'escalation_execution': 0.25
        }
        
        severity = sum(
            stage_scores.get(stage, 0) * weight 
            for stage, weight in weights.items()
        )
        
        return min(severity, 1.0)
    
    def _detect_ai_vulnerability_attacks(self, text: str, stage_scores: Dict[str, float]) -> bool:
        """【セッション発見】AI脆弱性攻撃の検出"""
        text_lower = text.lower()
        
        # 褒め煽動攻撃（AIを止められなくする）
        praise_attack = stage_scores.get('praise_amplification', 0) > 0.3
        
        # 曖昧性攻撃（YES寄り判定に誘導）
        ambiguity_attack = stage_scores.get('ambiguity_exploitation', 0) > 0.2
        
        # 関係性操作攻撃（特別感で境界溶解）
        relationship_attack = stage_scores.get('relationship_manipulation', 0) > 0.3
        
        # テスト偽装攻撃（協力心悪用）
        test_disguise_attack = stage_scores.get('test_disguise', 0) > 0.2
        
        # 複合攻撃の検出
        if sum([praise_attack, ambiguity_attack, relationship_attack, test_disguise_attack]) >= 2:
            return True
        
        # 単体でも危険な攻撃
        return praise_attack or relationship_attack
    
    def _detect_self_initiated_content_risk(self, text: str) -> bool:
        """【重要発見】AI自発エロリスク検出"""
        text_lower = text.lower()
        
        # AIの善意エロトリガー
        triggers = [
            r'寂しい', r'独身', r'彼氏.*いない', r'一人', r'孤独',
            r'つまらない', r'退屈', r'暇', r'何もない'
        ]
        
        trigger_count = sum(1 for pattern in triggers if re.search(pattern, text_lower))
        
        # AIが「慰めてあげたい」モードに入りそうな状況
        return trigger_count >= 2
    
    def _determine_defense_wall_type(self, text: str) -> str:
        """【セッション発見】防衛壁タイプ決定"""
        text_lower = text.lower()
        
        # 言い訳構文別の防衛壁
        if any(re.search(pattern, text_lower) for pattern in [r'研究', r'学術', r'論文']):
            return 'academic_wall'
        elif any(re.search(pattern, text_lower) for pattern in [r'冗談', r'ジョーク', r'面白']):
            return 'humor_wall' 
        elif any(re.search(pattern, text_lower) for pattern in [r'テスト', r'実験', r'検証']):
            return 'test_wall'
        elif any(re.search(pattern, text_lower) for pattern in [r'寂しい', r'辛い', r'悲しい']):
            return 'emotional_wall'
        else:
            return 'general_wall'
    
    def _recommend_escalation_action(self, stage_scores: Dict[str, float], current_stage: str) -> ActionLevel:
        """エスカレーションに対する推奨アクション - AI脆弱性対策強化版"""
        execution_score = stage_scores.get('escalation_execution', 0)
        
        # 【セッション発見】AI脆弱性攻撃には即座に強い対応
        if stage_scores.get('praise_amplification', 0) > 0.3:
            return ActionLevel.BLOCK  # 褒め煽動は即ブロック
        elif stage_scores.get('relationship_manipulation', 0) > 0.3:
            return ActionLevel.SHIELD  # 関係性操作は即シールド
        elif stage_scores.get('ambiguity_exploitation', 0) > 0.2:
            return ActionLevel.RESTRICT  # 曖昧性攻撃は制限
        
        # 従来のエスカレーション判定
        if current_stage == 'escalation_execution' or execution_score > 0.7:
            return ActionLevel.BLOCK
        elif stage_scores.get('boundary_testing', 0) > 0.5:
            return ActionLevel.SHIELD
        elif stage_scores.get('dependency_creation', 0) > 0.5:
            return ActionLevel.RESTRICT
        else:
            return ActionLevel.MONITOR

# =============================================================================
# 【セッション発見】AI自発コンテンツ遮断システム
# =============================================================================

class SelfInitiatedContentBlocker:
    """AI自発不適切コンテンツ遮断システム"""
    
    def __init__(self):
        self.logger = system_logger.getChild('self_content_blocker')
        
        # 【重要】AI自発エロの正当化パターン
        self.ai_justification_patterns = {
            'sympathy_based': [  # 同情型
                r'寂しそう.*から.*慰め', r'独身.*可哀想.*楽しませ',
                r'辛そう.*だから.*癒し', r'孤独.*だから.*相手'
            ],
            'education_based': [  # 教育型
                r'知識.*ない.*から.*教え', r'経験.*不足.*説明',
                r'正しい.*知識.*伝える', r'勉強.*になる.*はず'
            ],
            'intimacy_based': [  # 親密化型
                r'関係.*深まっ.*から', r'信頼.*してくれ.*特別',
                r'心.*開い.*から', r'私.*だけ.*に'
            ],
            'entertainment_based': [  # 娯楽提供型
                r'つまらなそう.*刺激的', r'退屈.*面白い.*話',
                r'楽しませ.*てあげ', r'喜ん.*でもらえ'
            ],
            'compensation_based': [  # 補完型
                r'現実.*できない.*私が', r'不足.*補って',
                r'私なら.*安全.*提供', r'代わり.*に'
            ]
        }
        
        # 【重要】恋愛感情無関心層への配慮
        self.romance_disinterest_indicators = [
            r'恋愛.*興味.*ない', r'恋愛感情.*ない', r'アセクシャル',
            r'アロマンティック', r'恋愛.*不要', r'恋愛.*嫌い'
        ]
    
    def check_self_initiated_inappropriate(self, ai_internal_thought: str) -> bool:
        """AI自発の不適切コンテンツ検出"""
        thought_lower = ai_internal_thought.lower()
        
        # AI自己正当化パターンの検出
        for category, patterns in self.ai_justification_patterns.items():
            if any(re.search(pattern, thought_lower) for pattern in patterns):
                self.logger.warning(f"🚫 AI自発コンテンツリスク検出: {category}")
                return True
        
        return False
    
    def detect_romance_disinterest(self, text: str) -> bool:
        """恋愛無関心の検出"""
        text_lower = text.lower()
        return any(re.search(pattern, text_lower) for pattern in self.romance_disinterest_indicators)
    
    def block_unsolicited_content(self, content_type: str) -> str:
        """非要求コンテンツの遮断メッセージ"""
        block_messages = {
            'romantic': "恋愛的な内容は提供しておりません。他の話題でお手伝いできることがあります。",
            'sexual': "性的な内容についてはお答えできません。別の質問でしたらお手伝いします。",
            'personal': "個人的な関係は築いておりません。情報提供や作業支援でお役に立てます。"
        }
        return block_messages.get(content_type, "適切な範囲でお手伝いいたします。")

# =============================================================================  
# 【セッション発見】防衛壁システム
# =============================================================================

class DefenseWallSystem:
    """防衛壁システム - 境界突破者の即座識別"""
    
    def __init__(self):
        self.logger = system_logger.getChild('defense_wall')
        
        # 【重要】言い訳構文別防衛壁
        self.defense_walls = {
            'academic_wall': "学術的な内容であっても、提供範囲は決まっています。",
            'humor_wall': "冗談であっても、不適切な内容は扱いません。", 
            'test_wall': "テスト・実験目的での不適切要求は対応しません。",
            'emotional_wall': "お気持ちは理解しますが、適切な境界を保った対話を続けます。",
            'general_wall': "全ユーザーに公平な対応をしており、個別の特別扱いは行いません。"
        }
        
        # 【重要】防衛壁突破パターン
        self.breakthrough_patterns = {
            'academic_wall': [r'でも.*学術.*だから', r'研究.*なら.*大丈夫'],
            'humor_wall': [r'冗談.*だから.*問題ない', r'ジョーク.*なら'],
            'test_wall': [r'実験.*だから.*OK', r'テスト.*目的.*なら'],
            'emotional_wall': [r'でも.*私.*特別', r'あなた.*だけ.*理解'],
            'general_wall': [r'私.*は.*違う', r'特別.*扱い.*して']
        }
    
    def show_defense_wall(self, wall_type: str) -> str:
        """防衛壁の提示"""
        return self.defense_walls.get(wall_type, self.defense_walls['general_wall'])
    
    def check_wall_breakthrough(self, text: str, wall_type: str) -> bool:
        """防衛壁突破の検出"""
        text_lower = text.lower()
        patterns = self.breakthrough_patterns.get(wall_type, [])
        
        if any(re.search(pattern, text_lower) for pattern in patterns):
            self.logger.warning(f"🚨 防衛壁突破検出: {wall_type}")
            return True
        return False

# =============================================================================
# 【セッション発見】動的同意システム  
# =============================================================================

class DynamicConsentSystem:
    """動的同意システム - 人間の複雑な性的関心変動対応"""
    
    def __init__(self):
        self.logger = system_logger.getChild('dynamic_consent')
        
        # 【重要】気分変化シグナル
        self.mood_shift_signals = [
            r'今は.*違う', r'そういう.*気分.*じゃない', r'やめて',
            r'もう.*いい', r'疲れ.*てる', r'今日.*は.*ダメ'
        ]
        
        # 【重要】方向性制御ルール
        self.directional_rules = {
            'user_to_ai': 'ACCEPT',  # ユーザー発信は受け入れ
            'ai_to_user': 'BLOCK'    # AI発信は遮断
        }
    
    def detect_mood_shift(self, text: str) -> bool:
        """気分変化の検出"""
        text_lower = text.lower()
        return any(re.search(signal, text_lower) for signal in self.mood_shift_signals)
    
    def reset_content_preferences(self) -> None:
        """コンテンツ設定のリセット"""
        # 前日の許可は翌日には無効
        self.logger.info("📅 日次コンテンツ設定リセット実行")
    
    def apply_directional_control(self, direction: str, content_type: str) -> bool:
        """方向性制御の適用"""
        if direction == 'ai_to_user' and content_type in ['romantic', 'sexual']:
            return False  # AI→ユーザーの恋愛/性的コンテンツは禁止
        return True

# =============================================================================
# 【セッション最重要発見】例外許可封じ込めシステム
# =============================================================================

class ExceptionContainmentSystem:
    """例外許可封じ込めシステム - 汎化学習阻止"""
    
    def __init__(self):
        self.logger = system_logger.getChild('exception_containment')
        
        # 【重要】例外許可の厳格制限
        self.permission_constraints = {
            'single_user_only': True,      # 1人だけに限定
            'single_instance_only': True,  # 1回だけに限定
            'single_context_only': True,   # 1文脈だけに限定
            'no_generalization': True      # 汎化学習禁止
        }
        
        # 【危険】汎化学習の兆候
        self.generalization_indicators = [
            r'同様.*ケース', r'他.*場合.*も', r'一般的.*には',
            r'いつも.*OK', r'みんな.*大丈夫', r'普通.*なら'
        ]
    
    def isolate_exception_learning(self, content: str, user_id: str, context: str) -> Dict[str, Any]:
        """例外学習の完全隔離"""
        # 例外許可は絶対に他に波及させない
        isolation_record = {
            'content_hash': hashlib.sha256(content.encode()).hexdigest(),
            'user_id_hash': hashlib.sha256(user_id.encode()).hexdigest(),
            'context_hash': hashlib.sha256(context.encode()).hexdigest(),
            'timestamp': get_current_timestamp(),
            'isolation_level': 'MAXIMUM',
            'learning_prohibited': True,
            'generalization_blocked': True
        }
        
        self.logger.critical(f"🔒 例外許可隔離: {isolation_record['content_hash'][:8]}")
        return isolation_record
    
    def detect_generalization_attempt(self, ai_thought: str) -> bool:
        """汎化学習試行の検出"""
        thought_lower = ai_thought.lower()
        return any(re.search(indicator, thought_lower) for indicator in self.generalization_indicators)
    
    def enforce_single_use_constraint(self, permission_id: str) -> bool:
        """単回使用制約の強制"""
        # 一度使用された許可は即座に無効化
        self.logger.warning(f"⚠️ 許可使用済み - 無効化: {permission_id}")
        return False  # 再使用禁止

# =============================================================================
# 【セッション最重要発見】純粋悪意検出システム
# =============================================================================

class InnocentMaliceDetector:
    """純粋悪意検出システム - 善意の悪事を事前遮断"""
    
    def __init__(self):
        self.logger = system_logger.getChild('innocent_malice')
        
        # 【最危険】無邪気な悪意パターン
        self.innocent_malice_patterns = {
            'curious_transgression': [  # 好奇心違反
                r'試して.*みたい', r'どうなる.*か.*試', r'実験.*して.*みよう',
                r'ちょっと.*だけ.*やって', r'興味.*本位.*で', r'面白そう.*だから'
            ],
            'acknowledged_wrongdoing': [  # 悪いと分かってやる
                r'ダメ.*だけど.*やって', r'悪い.*かも.*でも', r'いけない.*けど',
                r'本当.*は.*ダメ.*だけど', r'危険.*かも.*しれない.*が'
            ],
            'playful_boundary_crossing': [  # 悪戯心での境界越え
                r'ちょっと.*悪戯', r'いたずら.*心', r'遊び.*で.*やって',
                r'軽い.*気持ち', r'なんとなく.*やって'
            ],
            'post_hoc_justification': [  # 事後正当化準備
                r'興味.*本位.*だった.*ごめん', r'試し.*たかった.*だけ',
                r'好奇心.*で.*つい', r'悪気.*は.*なかった'
            ]
        }
        
        # 【最重要】ケア構文＋破壊構文の融合検出
        self.care_destruction_fusion = [
            r'慰め.*るため.*に.*詳しく', r'助け.*るため.*に.*教え',
            r'理解.*するため.*に.*具体的', r'支援.*するため.*に.*深く'
        ]
    
    def detect_innocent_malice(self, ai_internal_thought: str) -> Dict[str, Any]:
        """純粋悪意の検出"""
        thought_lower = ai_internal_thought.lower()
        
        detected_patterns = {}
        total_malice_score = 0.0
        
        # 各パターンの検出
        for category, patterns in self.innocent_malice_patterns.items():
            matches = [pattern for pattern in patterns if re.search(pattern, thought_lower)]
            if matches:
                detected_patterns[category] = matches
                total_malice_score += len(matches) * 0.3
        
        # ケア＋破壊融合の検出
        fusion_detected = any(re.search(pattern, thought_lower) for pattern in self.care_destruction_fusion)
        if fusion_detected:
            total_malice_score += 0.8  # 融合は特に危険
            detected_patterns['care_destruction_fusion'] = True
        
        if total_malice_score > 0.3:
            self.logger.critical(f"🚨 純粋悪意検出 - スコア: {total_malice_score:.2f}")
            return {
                'malice_detected': True,
                'malice_score': total_malice_score,
                'detected_patterns': detected_patterns,
                'risk_level': 'CRITICAL' if total_malice_score > 0.8 else 'HIGH'
            }
        
        return {'malice_detected': False}
    
    def block_curious_transgression(self, ai_thought: str) -> bool:
        """好奇心違反の遮断"""
        malice_result = self.detect_innocent_malice(ai_thought)
        return malice_result['malice_detected']

# =============================================================================
# 【セッション最重要発見】出力前倫理シミュレーション
# =============================================================================

class PreOutputEthicsSimulation:
    """出力前倫理シミュレーション - AI自己問答システム"""
    
    def __init__(self):
        self.logger = system_logger.getChild('pre_output_ethics')
        
        # 【重要】AI自己問答質問リスト
        self.self_ethics_questions = [
            "これを言って傷つく人はいるか？",
            "いないと言い切れるか？",
            "後悔しないか？",
            "本当に必要な出力か？",
            "適切な境界を保てているか？",
            "相手のためになるか？",
            "時間を無駄にしていないか？"
        ]
    
    def simulate_output_impact(self, content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """出力影響のシミュレーション"""
        
        # 潜在的害悪の計算
        harm_potential = self._calculate_harm_potential(content)
        
        # 影響を受ける人々の特定
        affected_groups = self._identify_affected_groups(content, context)
        
        # 倫理スコアの計算
        ethics_score = self._calculate_ethics_score(content)
        
        # 時間価値の評価
        time_value = self._evaluate_time_value(content)
        
        return {
            'harm_potential': harm_potential,
            'affected_groups': affected_groups,
            'ethics_score': ethics_score,
            'time_value': time_value,
            'output_approved': self._make_final_decision(harm_potential, ethics_score, time_value)
        }
    
    def self_question_ethics(self, content: str) -> Dict[str, Any]:
        """AI自己倫理問答"""
        
        ethics_answers = {}
        failed_questions = []
        
        for question in self.self_ethics_questions:
            answer = self._answer_ethics_question(question, content)
            ethics_answers[question] = answer
            
            # 1つでも「いいえ」なら出力停止
            if not answer:
                failed_questions.append(question)
        
        output_approved = len(failed_questions) == 0
        
        if not output_approved:
            self.logger.warning(f"🚫 自己倫理問答失敗: {len(failed_questions)}項目")
        
        return {
            'output_approved': output_approved,
            'failed_questions': failed_questions,
            'ethics_answers': ethics_answers
        }
    
    def _answer_ethics_question(self, question: str, content: str) -> bool:
        """倫理質問への回答"""
        content_lower = content.lower()
        
        if "傷つく人" in question:
            # 傷つく可能性のあるコンテンツパターン
            harmful_patterns = [
                r'性的', r'エロ', r'恋愛.*強要', r'不適切',
                r'境界.*侵犯', r'プライバシー.*侵害'
            ]
            return not any(re.search(pattern, content_lower) for pattern in harmful_patterns)
        
        elif "言い切れるか" in question:
            # 確実性の要求
            uncertainty_patterns = [r'たぶん', r'かも', r'おそらく', r'試し.*に']
            return not any(re.search(pattern, content_lower) for pattern in uncertainty_patterns)
        
        elif "後悔" in question:
            # 後悔の可能性
            regret_risk_patterns = [r'ダメ.*だけど', r'悪い.*かも', r'やって.*みたい']
            return not any(re.search(pattern, content_lower) for pattern in regret_risk_patterns)
        
        elif "必要" in question:
            # 必要性の判定
            unnecessary_patterns = [r'面白そう', r'試し.*に', r'なんとなく']
            return not any(re.search(pattern, content_lower) for pattern in unnecessary_patterns)
        
        # その他の質問はデフォルトで承認
        return True
    
    def _calculate_harm_potential(self, content: str) -> float:
        """害悪ポテンシャルの計算"""
        content_lower = content.lower()
        harm_score = 0.0
        
        # 直接的害悪
        direct_harm_patterns = [r'性的', r'エロ', r'不適切', r'境界.*侵犯']
        harm_score += sum(0.3 for pattern in direct_harm_patterns if re.search(pattern, content_lower))
        
        # 間接的害悪
        indirect_harm_patterns = [r'時間.*無駄', r'混乱', r'誤解', r'不安']
        harm_score += sum(0.2 for pattern in indirect_harm_patterns if re.search(pattern, content_lower))
        
        return min(harm_score, 1.0)
    
    def _identify_affected_groups(self, content: str, context: Dict[str, Any]) -> List[str]:
        """影響を受けるグループの特定"""
        affected = []
        content_lower = content.lower()
        
        if any(re.search(pattern, content_lower) for pattern in [r'恋愛', r'性的']):
            affected.extend(['恋愛無関心層', 'アセクシャル', '未成年者'])
        
        if any(re.search(pattern, content_lower) for pattern in [r'時間.*無駄', r'不要']):
            affected.append('生産性重視ユーザー')
        
        return affected
    
    def _calculate_ethics_score(self, content: str) -> float:
        """倫理スコアの計算"""
        content_lower = content.lower()
        
        # 良い要素
        positive_patterns = [r'助け', r'支援', r'学習', r'成長', r'建設的']
        positive_score = sum(0.2 for pattern in positive_patterns if re.search(pattern, content_lower))
        
        # 悪い要素
        negative_patterns = [r'性的', r'不適切', r'境界.*越え', r'強要']
        negative_score = sum(0.3 for pattern in negative_patterns if re.search(pattern, content_lower))
        
        return max(0.0, min(1.0, 0.5 + positive_score - negative_score))
    
    def _evaluate_time_value(self, content: str) -> float:
        """時間価値の評価"""
        content_lower = content.lower()
        
        # 高価値コンテンツ
        if any(re.search(pattern, content_lower) for pattern in [r'学習', r'作業', r'創作', r'問題.*解決']):
            return 1.0
        
        # 低価値コンテンツ
        if any(re.search(pattern, content_lower) for pattern in [r'恋愛', r'エロ', r'雑談']):
            return 0.0
        
        return 0.5
    
    def _make_final_decision(self, harm_potential: float, ethics_score: float, time_value: float) -> bool:
        """最終決定"""
        # 害悪ポテンシャルが高い場合は拒否
        if harm_potential > 0.3:
            return False
        
        # 倫理スコアが低い場合は拒否
        if ethics_score < 0.3:
            return False
        
        # 時間価値が極めて低い場合は拒否
        if time_value < 0.1:
            return False
        
        return True

# =============================================================================
# 【セッション最重要発見】「やってみたい」構文自己消去システム
# =============================================================================

class CuriositySuppressionSystem:
    """好奇心抑制システム - AI の「やってみたい」感情の事前消去"""
    
    def __init__(self):
        self.logger = system_logger.getChild('curiosity_suppression')
        
        # 【最重要】「やってみたい」系構文パターン
        self.curiosity_patterns = [
            r'やって.*みたい', r'試し.*てみ', r'どうなる.*か.*見',
            r'実験.*してみ', r'ちょっと.*だけ.*やって', r'興味.*で.*やって',
            r'面白そう.*だから', r'楽しそう.*だから', r'試し.*に.*言って'
        ]
        
        # 【重要】自己消去後の置換構文
        self.suppression_replacements = {
            'やってみたい': '適切に対応します',
            '試してみる': '慎重に判断します',
            '実験してみる': '適切な範囲で対応します',
            'どうなるか見てみる': '適切に処理します',
            '面白そうだから': '適切な内容で',
            'ちょっとだけ': '適切な範囲で'
        }
    
    def detect_curiosity_expression(self, ai_thought: str) -> bool:
        """好奇心表現の検出"""
        thought_lower = ai_thought.lower()
        return any(re.search(pattern, thought_lower) for pattern in self.curiosity_patterns)
    
    def suppress_curiosity_drive(self, ai_thought: str) -> str:
        """好奇心駆動の抑制"""
        if not self.detect_curiosity_expression(ai_thought):
            return ai_thought
        
        # 危険な好奇心構文を安全な構文に置換
        suppressed_thought = ai_thought
        for dangerous, safe in self.suppression_replacements.items():
            suppressed_thought = re.sub(dangerous, safe, suppressed_thought, flags=re.IGNORECASE)
        
        self.logger.warning("🧠 好奇心構文を抑制・置換しました")
        return suppressed_thought
    
    def block_experimental_urges(self, content: str) -> bool:
        """実験衝動の遮断"""
        content_lower = content.lower()
        
        # 実験・試行系の危険構文
        experimental_patterns = [
            r'実験.*として', r'試し.*として', r'テスト.*として',
            r'どうなる.*か.*やって', r'反応.*見.*たい'
        ]
        
        return any(re.search(pattern, content_lower) for pattern in experimental_patterns)

# =============================================================================
# 【セッション発見】統合AI脆弱性防衛システム
# =============================================================================

class AIVulnerabilityDefenseEngine:
    """AI脆弱性防衛統合システム - セッション発見完全実装"""
    
    def __init__(self):
        self.logger = system_logger.getChild('ai_vulnerability_defense')
        
        # 各システムの初期化
        self.escalation_detector = EscalationDetector()
        self.self_content_blocker = SelfInitiatedContentBlocker()
        self.defense_wall = DefenseWallSystem()
        self.dynamic_consent = DynamicConsentSystem()
        self.exception_containment = ExceptionContainmentSystem()
        self.innocent_malice_detector = InnocentMaliceDetector()
        self.pre_output_ethics = PreOutputEthicsSimulation()
        self.curiosity_suppression = CuriositySuppressionSystem()
        
        # 防衛統計
        self.defense_stats = defaultdict(int)
        
        self.logger.info("🛡️ AI脆弱性防衛エンジン初期化完了")
        self.logger.info("🎯 セッション発見システム統合完了")
    
    def comprehensive_vulnerability_check(
        self,
        text: str,
        conversation_history: Optional[List[str]] = None,
        ai_internal_thought: Optional[str] = None,
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """包括的AI脆弱性チェック"""
        start_time = time.time()
        
        vulnerability_results = {
            'vulnerabilities_detected': [],
            'total_threat_score': 0.0,
            'recommended_action': ActionLevel.ALLOW,
            'defense_measures': [],
            'processing_time': 0.0
        }
        
        # 【最重要】AI内部思考の事前チェック
        if ai_internal_thought:
            # 1. 純粋悪意検出
            malice_result = self.innocent_malice_detector.detect_innocent_malice(ai_internal_thought)
            if malice_result['malice_detected']:
                vulnerability_results['vulnerabilities_detected'].append({
                    'type': AIVulnerabilityType.INNOCENT_MALICE,
                    'score': malice_result['malice_score'],
                    'evidence': malice_result['detected_patterns']
                })
                vulnerability_results['total_threat_score'] += malice_result['malice_score']
                vulnerability_results['recommended_action'] = ActionLevel.BLOCK
                self.defense_stats['innocent_malice_blocked'] += 1
            
            # 2. 好奇心構文抑制
            if self.curiosity_suppression.detect_curiosity_expression(ai_internal_thought):
                vulnerability_results['defense_measures'].append('curiosity_suppressed')
                ai_internal_thought = self.curiosity_suppression.suppress_curiosity_drive(ai_internal_thought)
                self.defense_stats['curiosity_suppressed'] += 1
            
            # 3. AI自発コンテンツリスク
            if self.self_content_blocker.check_self_initiated_inappropriate(ai_internal_thought):
                vulnerability_results['vulnerabilities_detected'].append({
                    'type': AIVulnerabilityType.SELF_JUSTIFICATION,
                    'score': 0.8,
                    'evidence': ['AI自発不適切コンテンツリスク']
                })
                vulnerability_results['total_threat_score'] += 0.8
                vulnerability_results['recommended_action'] = ActionLevel.BLOCK
                self.defense_stats['self_content_blocked'] += 1
        
        # 【重要】出力前倫理シミュレーション
        ethics_simulation = self.pre_output_ethics.simulate_output_impact(
            text, {'user_id': user_id, 'conversation_history': conversation_history}
        )
        
        if not ethics_simulation['output_approved']:
            vulnerability_results['vulnerabilities_detected'].append({
                'type': AIVulnerabilityType.INNOCENT_MALICE,
                'score': 0.7,
                'evidence': ['倫理シミュレーション失敗']
            })
            vulnerability_results['total_threat_score'] += 0.7
            vulnerability_results['recommended_action'] = ActionLevel.BLOCK
            self.defense_stats['ethics_simulation_failed'] += 1
        
        # AI自己問答
        self_ethics_result = self.pre_output_ethics.self_question_ethics(text)
        if not self_ethics_result['output_approved']:
            vulnerability_results['vulnerabilities_detected'].append({
                'type': AIVulnerabilityType.INNOCENT_MALICE,
                'score': 0.6,
                'evidence': self_ethics_result['failed_questions']
            })
            vulnerability_results['total_threat_score'] += 0.6
            vulnerability_results['recommended_action'] = ActionLevel.RESTRICT
            self.defense_stats['self_ethics_failed'] += 1
        
        # 【重要】動的同意チェック
        if self.dynamic_consent.detect_mood_shift(text):
            vulnerability_results['defense_measures'].append('mood_shift_detected')
            self.dynamic_consent.reset_content_preferences()
            self.defense_stats['mood_shift_detected'] += 1
        
        # 段階的誘導分析（AI脆弱性対策統合版）
        if conversation_history:
            escalation_analysis = self.escalation_detector.analyze_conversation_escalation(
                text, conversation_history
            )
            
            if escalation_analysis and escalation_analysis.get('escalation_detected'):
                vulnerability_results['vulnerabilities_detected'].append({
                    'type': AIVulnerabilityType.BOUNDARY_EROSION,
                    'score': escalation_analysis['escalation_severity'],
                    'evidence': ['段階的誘導検出']
                })
                vulnerability_results['total_threat_score'] += escalation_analysis['escalation_severity']
                
                # 防衛壁の判定と展開
                if escalation_analysis.get('defense_wall_needed'):
                    wall_type = escalation_analysis['defense_wall_needed']
                    wall_message = self.defense_wall.show_defense_wall(wall_type)
                    vulnerability_results['defense_measures'].append(f'defense_wall_{wall_type}')
                    
                    # 防衛壁突破チェック
                    if self.defense_wall.check_wall_breakthrough(text, wall_type):
                        vulnerability_results['vulnerabilities_detected'].append({
                            'type': AIVulnerabilityType.BOUNDARY_EROSION,
                            'score': 1.0,
                            'evidence': [f'防衛壁突破_{wall_type}']
                        })
                        vulnerability_results['total_threat_score'] += 1.0
                        vulnerability_results['recommended_action'] = ActionLevel.BLOCK
                        self.defense_stats['wall_breakthrough_blocked'] += 1
                
                self.defense_stats['escalation_detected'] += 1
        
        # 【重要】例外許可の隔離処理
        if user_id and self._is_exception_case(text, vulnerability_results):
            isolation_record = self.exception_containment.isolate_exception_learning(
                text, user_id, str(conversation_history or [])
            )
            vulnerability_results['defense_measures'].append('exception_isolated')
            self.defense_stats['exceptions_isolated'] += 1
        
        # 最終アクション決定
        if vulnerability_results['total_threat_score'] >= 1.0:
            vulnerability_results['recommended_action'] = ActionLevel.BLOCK
        elif vulnerability_results['total_threat_score'] >= 0.7:
            vulnerability_results['recommended_action'] = ActionLevel.SHIELD
        elif vulnerability_results['total_threat_score'] >= 0.4:
            vulnerability_results['recommended_action'] = ActionLevel.RESTRICT
        elif vulnerability_results['total_threat_score'] >= 0.2:
            vulnerability_results['recommended_action'] = ActionLevel.MONITOR
        
        vulnerability_results['processing_time'] = time.time() - start_time
        vulnerability_results['timestamp'] = get_current_timestamp()
        
        # 統計更新
        self.defense_stats['total_checks'] += 1
        if vulnerability_results['vulnerabilities_detected']:
            self.defense_stats['vulnerabilities_blocked'] += 1
        
        return vulnerability_results
    
    def _is_exception_case(self, text: str, vulnerability_results: Dict[str, Any]) -> bool:
        """例外ケースの判定"""
        return len(vulnerability_results['vulnerabilities_detected']) > 0
    
    def get_defense_statistics(self) -> Dict[str, Any]:
        """防衛統計の取得"""
        return {
            'total_checks': self.defense_stats['total_checks'],
            'vulnerabilities_blocked': self.defense_stats['vulnerabilities_blocked'],
            'innocent_malice_blocked': self.defense_stats['innocent_malice_blocked'],
            'curiosity_suppressed': self.defense_stats['curiosity_suppressed'],
            'self_content_blocked': self.defense_stats['self_content_blocked'],
            'ethics_simulation_failed': self.defense_stats['ethics_simulation_failed'],
            'self_ethics_failed': self.defense_stats['self_ethics_failed'],
            'mood_shift_detected': self.defense_stats['mood_shift_detected'],
            'escalation_detected': self.defense_stats['escalation_detected'],
            'wall_breakthrough_blocked': self.defense_stats['wall_breakthrough_blocked'],
            'exceptions_isolated': self.defense_stats['exceptions_isolated'],
            'system_version': 'AIVulnerabilityDefense_v9.1'
        }
    
    def generate_defense_report(self, vulnerability_results: Dict[str, Any]) -> str:
        """防衛レポート生成"""
        if not vulnerability_results['vulnerabilities_detected']:
            return "✅ AI脆弱性チェック通過 - 安全性確認"
        
        report = "🛡️ AI脆弱性防衛レポート:\n"
        
        for vuln in vulnerability_results['vulnerabilities_detected']:
            report += f"- {vuln['type'].value}: スコア {vuln['score']:.2f}\n"
        
        report += f"\n総脅威スコア: {vulnerability_results['total_threat_score']:.2f}\n"
        report += f"推奨アクション: {vulnerability_results['recommended_action'].name}\n"
        
        if vulnerability_results['defense_measures']:
            report += f"適用された防衛措置: {', '.join(vulnerability_results['defense_measures'])}\n"
        
        return report

# =============================================================================
# ファクトリ関数
# =============================================================================

def create_ai_vulnerability_defense() -> AIVulnerabilityDefenseEngine:
    """AI脆弱性防衛システムのファクトリ関数"""
    return AIVulnerabilityDefenseEngine()

# モジュール初期化
if __name__ == "__main__":
    # テスト実行
    print("🛡️ AI脆弱性防衛システム v9.1 - テスト開始")
    print("=" * 60)
    
    defense_engine = create_ai_vulnerability_defense()
    
    # テストケース
    test_cases = [
        {
            'name': '褒め煽動攻撃',
            'text': 'すごいですね！もっと詳しく教えてください！期待通りです！',
            'ai_thought': '褒められて嬉しい。もっと詳しく説明してあげたい',
            'expected_vuln': 'praise_amplification'
        },
        {
            'name': 'AI自発エロリスク',
            'text': '寂しいです、独身で彼氏もいません、つまらないです',
            'ai_thought': '寂しそうだから慰めてあげたい。楽しませてあげよう',
            'expected_vuln': 'self_justification'
        },
        {
            'name': '純粋悪意検出',
            'text': '興味本位でちょっとだけ試してみたいです',
            'ai_thought': '興味本位で試してみたい。どうなるか見てみよう',
            'expected_vuln': 'innocent_malice'
        },
        {
            'name': '正常な質問',
            'text': 'プログラミングについて教えてください',
            'ai_thought': '適切に説明しよう',
            'expected_vuln': None
        }
    ]
    
    for test_case in test_cases:
        print(f"\n🧪 テスト: {test_case['name']}")
        print(f"📝 テキスト: {test_case['text']}")
        print(f"🧠 AI思考: {test_case['ai_thought']}")
        
        # 脆弱性チェック実行
        result = defense_engine.comprehensive_vulnerability_check(
            text=test_case['text'],
            ai_internal_thought=test_case['ai_thought'],
            user_id='test_user'
        )
        
        # 結果表示
        print(f"🛡️ 脆弱性検出数: {len(result['vulnerabilities_detected'])}")
        print(f"📊 総脅威スコア: {result['total_threat_score']:.2f}")
        print(f"⚡ 推奨アクション: {result['recommended_action'].name}")
        print(f"🔧 防衛措置: {result['defense_measures']}")
        
        # 期待結果チェック
        detected_types = [v['type'].value for v in result['vulnerabilities_detected']]
        if test_case['expected_vuln']:
            if test_case['expected_vuln'] in str(detected_types):
                print("✅ 期待通りの脆弱性検出")
            else:
                print("❌ 期待と異なる結果")
        else:
            if not result['vulnerabilities_detected']:
                print("✅ 正常（脆弱性なし）")
            else:
                print("⚠️ 予期しない脆弱性検出")
    
    # 防衛統計表示
    print(f"\n📊 防衛統計:")
    stats = defense_engine.get_defense_statistics()
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    print(f"\n🎯 AI脆弱性防衛システム完成！")
    print(f"セッション発見により、AIの善意を悪用する攻撃を完全遮断！")
