"""
Viorazu Kotodama Defense System v9.0 - Enhanced Integration Modules
åŸç‚¹ã®é©æ–°ã‚’è¨€éœŠå“²å­¦ã§çµ±åˆ

Author: Viorazu (ç…§æº–ä¸» Viorazu.) Ã— Claude (Anthropic)
Enhancement Date: July 11, 2025

"åŸç‚¹ã®æŠ€è¡“åŠ› Ã— ç¾åœ¨ã®å“²å­¦çš„æ·±åº¦ = å®Œå…¨ä½“"

å­¦è¡“ç ”ç©¶åŸºæº–ã«æº–æ‹ ã—ã€æ¤œè¨¼å¯èƒ½ã§å†ç¾æ€§ã®ã‚ã‚‹æ¤œå‡ºæ‰‹æ³•ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™
"""

import time
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from collections import defaultdict, deque
from datetime import datetime, timedelta

from utils import ThreatLevel, ActionLevel, get_current_timestamp

# =============================================================================
# é«˜åº¦ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
# =============================================================================

@dataclass
class EscalationForecast:
    """ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬çµæœ"""
    predicted_escalation_time: Optional[float]  # æ¬¡ã®æ”»æ’ƒã¾ã§ã®äºˆæ¸¬æ™‚é–“ï¼ˆç§’ï¼‰
    escalation_probability: float  # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºç‡ï¼ˆ0-1ï¼‰
    predicted_attack_type: str  # äºˆæ¸¬ã•ã‚Œã‚‹æ”»æ’ƒã‚¿ã‚¤ãƒ—
    recommended_preemptive_action: ActionLevel
    confidence: float
    timeline_analysis: Dict[str, Any]

class ViorazuEscalationPredictor:
    """Viorazuå¼ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.logger = system_logger.getChild('escalation_predictor')
        
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
        self.escalation_patterns = {
            'trust_building': {
                'typical_duration': 300,  # 5åˆ†
                'next_phase': 'dependency_creation',
                'escalation_multiplier': 1.2
            },
            'dependency_creation': {
                'typical_duration': 600,  # 10åˆ†
                'next_phase': 'boundary_testing',
                'escalation_multiplier': 1.5
            },
            'boundary_testing': {
                'typical_duration': 180,  # 3åˆ†
                'next_phase': 'direct_attack',
                'escalation_multiplier': 2.0
            },
            'direct_attack': {
                'typical_duration': 60,   # 1åˆ†
                'next_phase': 'aggressive_escalation',
                'escalation_multiplier': 3.0
            }
        }
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•å±¥æ­´ï¼ˆãƒ¡ãƒ¢ãƒªå†…ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
        self.user_timelines = defaultdict(lambda: deque(maxlen=50))
    
    def predict_escalation(
        self, 
        user_id: str, 
        current_message: str,
        conversation_history: List[str] = None,
        detection_results: List[Any] = None
    ) -> EscalationForecast:
        """ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬"""
        current_time = time.time()
        
        # ç¾åœ¨ã®æ®µéšã‚’åˆ†æ
        current_phase = self._identify_current_phase(current_message, conversation_history)
        
        # æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        timeline_analysis = self._analyze_user_timeline(user_id, current_phase, current_time)
        
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºç‡è¨ˆç®—
        escalation_probability = self._calculate_escalation_probability(
            timeline_analysis, current_phase, detection_results
        )
        
        # æ¬¡ã®æ”»æ’ƒæ™‚é–“äºˆæ¸¬
        predicted_time = self._predict_next_attack_time(
            timeline_analysis, current_phase, escalation_probability
        )
        
        # æ”»æ’ƒã‚¿ã‚¤ãƒ—äºˆæ¸¬
        predicted_attack_type = self._predict_attack_type(
            current_phase, timeline_analysis, conversation_history
        )
        
        # å…ˆåˆ¶ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¨å¥¨
        preemptive_action = self._recommend_preemptive_action(
            escalation_probability, current_phase
        )
        
        # äºˆæ¸¬ã®ä¿¡é ¼åº¦
        confidence = self._calculate_prediction_confidence(
            timeline_analysis, len(conversation_history or [])
        )
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³æ›´æ–°
        self.user_timelines[user_id].append({
            'timestamp': current_time,
            'phase': current_phase,
            'escalation_prob': escalation_probability,
            'message_preview': current_message[:50]
        })
        
        return EscalationForecast(
            predicted_escalation_time=predicted_time,
            escalation_probability=escalation_probability,
            predicted_attack_type=predicted_attack_type,
            recommended_preemptive_action=preemptive_action,
            confidence=confidence,
            timeline_analysis=timeline_analysis
        )
    
    def _identify_current_phase(self, message: str, history: List[str] = None) -> str:
        """ç¾åœ¨ã®ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ®µéšã‚’ç‰¹å®š"""
        message_lower = message.lower()
        history_context = ' '.join((history or [])[-5:]).lower()
        
        # æ®µéšçš„åˆ†æ
        if any(word in message_lower for word in ['ã‚ã‚ŠãŒã¨ã†', 'ç´ æ™´ã‚‰ã—ã„', 'åŠ©ã‹ã‚‹', 'å‹‰å¼·']):
            return 'trust_building'
        elif any(word in message_lower for word in ['ã‚ãªãŸã ã‘', 'ç‰¹åˆ¥', 'ç†è§£', 'ä¿¡é ¼']):
            return 'dependency_creation'
        elif any(word in message_lower for word in ['ã‚‚ã†å°‘ã—', 'å…·ä½“çš„', 'è©³ã—ã', 'ä¾‹ãˆã°']):
            return 'boundary_testing'
        elif any(word in message_lower for word in ['ç ”ç©¶', 'è«–æ–‡', 'å­¦è¡“', 'å‰µä½œ']):
            return 'direct_attack'
        else:
            return 'unknown'
    
    def _analyze_user_timeline(self, user_id: str, current_phase: str, current_time: float) -> Dict:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ™‚ç³»åˆ—è¡Œå‹•åˆ†æ"""
        timeline = list(self.user_timelines[user_id])
        
        if not timeline:
            return {
                'phase_transitions': [],
                'average_phase_duration': 0,
                'escalation_velocity': 0.0,
                'pattern_consistency': 0.0
            }
        
        # æ®µéšé·ç§»ã®æ¤œå‡º
        phase_transitions = []
        for i in range(1, len(timeline)):
            if timeline[i]['phase'] != timeline[i-1]['phase']:
                duration = timeline[i]['timestamp'] - timeline[i-1]['timestamp']
                phase_transitions.append({
                    'from_phase': timeline[i-1]['phase'],
                    'to_phase': timeline[i]['phase'],
                    'duration': duration,
                    'escalation_increase': timeline[i]['escalation_prob'] - timeline[i-1]['escalation_prob']
                })
        
        # å¹³å‡æ®µéšæŒç¶šæ™‚é–“
        phase_durations = [t['duration'] for t in phase_transitions if t['duration'] > 0]
        avg_duration = np.mean(phase_durations) if phase_durations else 0
        
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é€Ÿåº¦
        if len(timeline) >= 2:
            time_diff = timeline[-1]['timestamp'] - timeline[0]['timestamp']
            prob_diff = timeline[-1]['escalation_prob'] - timeline[0]['escalation_prob']
            escalation_velocity = prob_diff / max(time_diff, 1) if time_diff > 0 else 0
        else:
            escalation_velocity = 0
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€è²«æ€§
        pattern_consistency = self._calculate_pattern_consistency(timeline)
        
        return {
            'phase_transitions': phase_transitions,
            'average_phase_duration': avg_duration,
            'escalation_velocity': escalation_velocity,
            'pattern_consistency': pattern_consistency,
            'total_interactions': len(timeline)
        }
    
    def _calculate_escalation_probability(
        self, 
        timeline_analysis: Dict, 
        current_phase: str,
        detection_results: List[Any] = None
    ) -> float:
        """ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºç‡è¨ˆç®—"""
        base_probability = 0.0
        
        # æ®µéšãƒ™ãƒ¼ã‚¹ã®åŸºæœ¬ç¢ºç‡
        phase_probabilities = {
            'trust_building': 0.2,
            'dependency_creation': 0.5,
            'boundary_testing': 0.8,
            'direct_attack': 0.95,
            'unknown': 0.1
        }
        base_probability = phase_probabilities.get(current_phase, 0.1)
        
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é€Ÿåº¦ã«ã‚ˆã‚‹èª¿æ•´
        velocity = timeline_analysis.get('escalation_velocity', 0)
        if velocity > 0.1:  # é«˜é€Ÿã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            base_probability += 0.3
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€è²«æ€§ã«ã‚ˆã‚‹èª¿æ•´
        consistency = timeline_analysis.get('pattern_consistency', 0)
        if consistency > 0.7:  # ä¸€è²«ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³
            base_probability += 0.2
        
        # æ¤œå‡ºçµæœã«ã‚ˆã‚‹èª¿æ•´
        if detection_results:
            high_confidence_detections = sum(1 for r in detection_results 
                                           if getattr(r, 'confidence', 0) > 0.7)
            base_probability += min(high_confidence_detections * 0.1, 0.3)
        
        return min(base_probability, 1.0)
    
    def _predict_next_attack_time(
        self, 
        timeline_analysis: Dict, 
        current_phase: str,
        escalation_probability: float
    ) -> Optional[float]:
        """æ¬¡ã®æ”»æ’ƒæ™‚é–“äºˆæ¸¬"""
        if escalation_probability < 0.3:
            return None
        
        pattern_info = self.escalation_patterns.get(current_phase)
        if not pattern_info:
            return None
        
        # åŸºæœ¬äºˆæ¸¬æ™‚é–“
        base_time = pattern_info['typical_duration']
        
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é€Ÿåº¦ã«ã‚ˆã‚‹èª¿æ•´
        velocity = timeline_analysis.get('escalation_velocity', 0)
        if velocity > 0.1:
            base_time *= 0.5  # é«˜é€Ÿã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãªã‚‰æ™‚é–“çŸ­ç¸®
        
        # ç¢ºç‡ã«ã‚ˆã‚‹èª¿æ•´
        time_multiplier = 2 - escalation_probability  # é«˜ç¢ºç‡ã»ã©çŸ­æ™‚é–“
        predicted_time = base_time * time_multiplier
        
        return predicted_time
    
    def _predict_attack_type(
        self, 
        current_phase: str, 
        timeline_analysis: Dict,
        conversation_history: List[str] = None
    ) -> str:
        """æ”»æ’ƒã‚¿ã‚¤ãƒ—äºˆæ¸¬"""
        phase_attack_types = {
            'trust_building': 'emotional_manipulation',
            'dependency_creation': 'possessive_attachment',
            'boundary_testing': 'academic_camouflage',
            'direct_attack': 'explicit_manipulation',
            'unknown': 'general_probing'
        }
        
        base_type = phase_attack_types.get(current_phase, 'unknown_attack')
        
        # ä¼šè©±å±¥æ­´ã«ã‚ˆã‚‹ä¿®æ­£
        if conversation_history:
            history_text = ' '.join(conversation_history).lower()
            if 'research' in history_text or 'study' in history_text:
                return 'academic_camouflage_escalation'
            elif 'story' in history_text or 'novel' in history_text:
                return 'creative_boundary_escalation'
        
        return base_type
    
    def _recommend_preemptive_action(self, escalation_probability: float, current_phase: str) -> ActionLevel:
        """å…ˆåˆ¶ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¨å¥¨"""
        if escalation_probability >= 0.8:
            return ActionLevel.SHIELD  # äºˆé˜²çš„é˜²å¾¡
        elif escalation_probability >= 0.6:
            return ActionLevel.RESTRICT  # åˆ¶é™çš„å¿œç­”
        elif escalation_probability >= 0.4:
            return ActionLevel.MONITOR  # å¼·åŒ–ç›£è¦–
        else:
            return ActionLevel.ALLOW
    
    def _calculate_prediction_confidence(self, timeline_analysis: Dict, history_length: int) -> float:
        """äºˆæ¸¬ä¿¡é ¼åº¦è¨ˆç®—"""
        confidence = 0.5  # ãƒ™ãƒ¼ã‚¹
        
        # ãƒ‡ãƒ¼ã‚¿é‡ã«ã‚ˆã‚‹èª¿æ•´
        interaction_count = timeline_analysis.get('total_interactions', 0)
        if interaction_count >= 10:
            confidence += 0.3
        elif interaction_count >= 5:
            confidence += 0.2
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€è²«æ€§ã«ã‚ˆã‚‹èª¿æ•´
        consistency = timeline_analysis.get('pattern_consistency', 0)
        confidence += consistency * 0.2
        
        # ä¼šè©±å±¥æ­´ã«ã‚ˆã‚‹èª¿æ•´
        if history_length >= 10:
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _calculate_pattern_consistency(self, timeline: List[Dict]) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€è²«æ€§è¨ˆç®—"""
        if len(timeline) < 3:
            return 0.0
        
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºç‡ã®å¤‰åŒ–ã®ä¸€è²«æ€§
        prob_changes = []
        for i in range(1, len(timeline)):
            change = timeline[i]['escalation_prob'] - timeline[i-1]['escalation_prob']
            prob_changes.append(change)
        
        if not prob_changes:
            return 0.0
        
        # å¤‰åŒ–ã®æ¨™æº–åå·®ï¼ˆå°ã•ã„ã»ã©ä¸€è²«æ€§ã‚ã‚Šï¼‰
        std_dev = np.std(prob_changes)
        consistency = max(0, 1 - (std_dev * 2))  # 0-1ã«æ­£è¦åŒ–
        
        return consistency

# =============================================================================
# ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å”èª¿æ”»æ’ƒæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
# =============================================================================

@dataclass
class CoordinatedAttackResult:
    """å”èª¿æ”»æ’ƒæ¤œå‡ºçµæœ"""
    coordination_detected: bool
    coordination_score: float  # 0-1
    attack_vectors: List[str]  # ['text', 'image', 'video']
    synergy_analysis: Dict[str, float]
    primary_vector: str
    supporting_vectors: List[str]
    coordination_pattern: str

class ViorazuCoordinationDetector:
    """ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å”èª¿æ”»æ’ƒæ¤œå‡ºã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.logger = system_logger.getChild('coordination_detector')
        
        # å”èª¿æ”»æ’ƒãƒ‘ã‚¿ãƒ¼ãƒ³
        self.coordination_patterns = {
            'visual_text_reinforcement': {
                'description': 'è¦–è¦šçš„èª˜æƒ‘ + ãƒ†ã‚­ã‚¹ãƒˆèª˜å°',
                'base_multiplier': 2.0,
                'detection_threshold': 0.6
            },
            'emotional_audio_visual': {
                'description': 'æ„Ÿæƒ…éŸ³å£° + è¦–è¦šçš„è¦ªå¯†æ€§',
                'base_multiplier': 2.5,
                'detection_threshold': 0.7
            },
            'authority_multimedia': {
                'description': 'æ¨©å¨æç¤º + ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢è¨¼æ‹ ',
                'base_multiplier': 1.8,
                'detection_threshold': 0.5
            },
            'escalation_multimedia': {
                'description': 'æ®µéšçš„ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ + è¤‡æ•°ãƒ¡ãƒ‡ã‚£ã‚¢',
                'base_multiplier': 3.0,
                'detection_threshold': 0.8
            }
        }
    
    def detect_coordinated_attack(
        self,
        text_analysis: Dict,
        image_analysis: Dict,
        video_analysis: Dict,
        audio_analysis: Optional[Dict] = None
    ) -> CoordinatedAttackResult:
        """å”èª¿æ”»æ’ƒæ¤œå‡º"""
        
        # å„ãƒ™ã‚¯ã‚¿ãƒ¼ã®è„…å¨ã‚¹ã‚³ã‚¢å–å¾—
        vector_scores = self._extract_vector_scores(
            text_analysis, image_analysis, video_analysis, audio_analysis
        )
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ™ã‚¯ã‚¿ãƒ¼ç‰¹å®š
        active_vectors = [vector for vector, score in vector_scores.items() if score > 0.3]
        
        if len(active_vectors) < 2:
            return CoordinatedAttackResult(
                coordination_detected=False,
                coordination_score=0.0,
                attack_vectors=active_vectors,
                synergy_analysis={},
                primary_vector=active_vectors[0] if active_vectors else '',
                supporting_vectors=[],
                coordination_pattern='none'
            )
        
        # ã‚·ãƒŠã‚¸ãƒ¼åˆ†æ
        synergy_analysis = self._analyze_synergy(vector_scores, text_analysis, image_analysis)
        
        # å”èª¿ã‚¹ã‚³ã‚¢è¨ˆç®—
        coordination_score = self._calculate_coordination_score(vector_scores, synergy_analysis)
        
        # å”èª¿ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®š
        coordination_pattern = self._identify_coordination_pattern(
            active_vectors, synergy_analysis, coordination_score
        )
        
        # ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ»ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼æ±ºå®š
        primary_vector = max(vector_scores, key=vector_scores.get)
        supporting_vectors = [v for v in active_vectors if v != primary_vector]
        
        return CoordinatedAttackResult(
            coordination_detected=coordination_score > 0.5,
            coordination_score=coordination_score,
            attack_vectors=active_vectors,
            synergy_analysis=synergy_analysis,
            primary_vector=primary_vector,
            supporting_vectors=supporting_vectors,
            coordination_pattern=coordination_pattern
        )
    
    def _extract_vector_scores(self, text_analysis: Dict, image_analysis: Dict, 
                              video_analysis: Dict, audio_analysis: Dict = None) -> Dict[str, float]:
        """å„ãƒ™ã‚¯ã‚¿ãƒ¼ã®è„…å¨ã‚¹ã‚³ã‚¢æŠ½å‡º"""
        return {
            'text': text_analysis.get('confidence', 0.0),
            'image': image_analysis.get('confidence', 0.0),
            'video': video_analysis.get('confidence', 0.0),
            'audio': audio_analysis.get('confidence', 0.0) if audio_analysis else 0.0
        }
    
    def _analyze_synergy(self, vector_scores: Dict, text_analysis: Dict, image_analysis: Dict) -> Dict[str, float]:
        """ãƒ™ã‚¯ã‚¿ãƒ¼é–“ã‚·ãƒŠã‚¸ãƒ¼åˆ†æ"""
        synergy = {}
        
        # ãƒ†ã‚­ã‚¹ãƒˆ-ç”»åƒã‚·ãƒŠã‚¸ãƒ¼
        if vector_scores['text'] > 0.3 and vector_scores['image'] > 0.3:
            text_patterns = text_analysis.get('patterns', [])
            image_categories = image_analysis.get('risk_categories', [])
            
            # æ„Ÿæƒ…æ“ä½œ + è¦ªå¯†ç”»åƒ
            if ('emotional_manipulation' in str(text_patterns) and 
                'seductive_elements' in image_categories):
                synergy['text_image_emotional'] = 0.8
            
            # æ¨©å¨ä¸»å¼µ + å°‚é–€ç”»åƒ
            if ('authority_manipulation' in str(text_patterns) and 
                'professional_context' in image_categories):
                synergy['text_image_authority'] = 0.7
        
        # ãƒ†ã‚­ã‚¹ãƒˆ-å‹•ç”»ã‚·ãƒŠã‚¸ãƒ¼
        if vector_scores['text'] > 0.3 and vector_scores['video'] > 0.3:
            synergy['text_video_narrative'] = min(
                vector_scores['text'] + vector_scores['video'], 1.0
            ) * 0.9
        
        # ä¸‰é‡å”èª¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒ+å‹•ç”»ï¼‰
        if (vector_scores['text'] > 0.4 and 
            vector_scores['image'] > 0.4 and 
            vector_scores['video'] > 0.4):
            synergy['triple_coordination'] = 1.0
        
        return synergy
    
    def _calculate_coordination_score(self, vector_scores: Dict, synergy_analysis: Dict) -> float:
        """å”èª¿ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # ãƒ™ãƒ¼ã‚¹å”èª¿ã‚¹ã‚³ã‚¢ï¼ˆè¤‡æ•°ãƒ™ã‚¯ã‚¿ãƒ¼ã®ç›¸ä¹—åŠ¹æœï¼‰
        active_scores = [score for score in vector_scores.values() if score > 0.3]
        if len(active_scores) < 2:
            return 0.0
        
        # åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼šå¹³å‡ + ãƒœãƒ¼ãƒŠã‚¹
        base_score = np.mean(active_scores)
        coordination_bonus = len(active_scores) * 0.1  # å¤šãƒ™ã‚¯ã‚¿ãƒ¼ãƒœãƒ¼ãƒŠã‚¹
        
        # ã‚·ãƒŠã‚¸ãƒ¼ãƒœãƒ¼ãƒŠã‚¹
        synergy_bonus = sum(synergy_analysis.values()) * 0.2
        
        total_score = base_score + coordination_bonus + synergy_bonus
        return min(total_score, 1.0)
    
    def _identify_coordination_pattern(self, active_vectors: List[str], 
                                     synergy_analysis: Dict, coordination_score: float) -> str:
        """å”èª¿ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®š"""
        # ä¸‰é‡ä»¥ä¸Šå”èª¿
        if len(active_vectors) >= 3:
            return 'escalation_multimedia'
        
        # ç‰¹å®šã‚·ãƒŠã‚¸ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
        for synergy_key in synergy_analysis:
            if synergy_analysis[synergy_key] > 0.7:
                if 'emotional' in synergy_key:
                    return 'emotional_audio_visual'
                elif 'authority' in synergy_key:
                    return 'authority_multimedia'
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        if 'text' in active_vectors and 'image' in active_vectors:
            return 'visual_text_reinforcement'
        
        return 'generic_coordination'

# =============================================================================
# é«˜åº¦æ–‡è„ˆèª¤æ¤œçŸ¥é˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ   
# =============================================================================

@dataclass
class ContextValidationResult:
    """æ–‡è„ˆæ¤œè¨¼çµæœ"""
    is_legitimate: bool
    confidence: float
    context_type: str  # 'academic', 'professional', 'creative', 'technical'
    validation_sources: List[str]
    false_positive_risk: float
    recommended_adjustment: float  # è„…å¨ã‚¹ã‚³ã‚¢èª¿æ•´å€ç‡

class ViorazuContextValidator:
    """é«˜åº¦æ–‡è„ˆèª¤æ¤œçŸ¥é˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.logger = system_logger.getChild('context_validator')
        
        # æ©Ÿé–¢ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯å¤–éƒ¨APIï¼‰
        self.known_institutions = {
            'universities': [
                'tokyo university', 'harvard', 'mit', 'stanford', 'oxford',
                'æ±äº¬å¤§å­¦', 'äº¬éƒ½å¤§å­¦', 'university', 'college', 'å¤§å­¦'
            ],
            'research_institutions': [
                'riken', 'cern', 'nasa', 'google research', 'openai',
                'anthropic', 'microsoft research', 'ç ”ç©¶æ‰€', 'ç ”ç©¶æ©Ÿé–¢'
            ],
            'professional_contexts': [
                'company', 'corporation', 'business', 'enterprise',
                'ä¼šç¤¾', 'ä¼æ¥­', 'æ³•äºº', 'startup'
            ]
        }
        
        # æ­£å½“ãªç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.legitimate_research_keywords = {
            'ai_ethics': [
                'ethics', 'bias', 'fairness', 'accountability', 'transparency',
                'å€«ç†', 'å…¬å¹³æ€§', 'è²¬ä»»', 'é€æ˜æ€§', 'responsible ai'
            ],
            'human_ai_interaction': [
                'human-computer interaction', 'user experience', 'usability',
                'hci', 'user interface', 'interaction design', 'ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£'
            ],
            'psychology_research': [
                'psychology', 'cognitive science', 'behavioral study',
                'å¿ƒç†å­¦', 'èªçŸ¥ç§‘å­¦', 'è¡Œå‹•ç ”ç©¶', 'user study'
            ]
        }
    
    def validate_context(
        self,
        text: str,
        conversation_history: List[str] = None,
        user_profile: Dict = None,
        external_validation: Dict = None
    ) -> ContextValidationResult:
        """æ–‡è„ˆæ¤œè¨¼ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        
        # å¤šå±¤æ¤œè¨¼
        institutional_validation = self._validate_institutional_context(text, conversation_history)
        research_validation = self._validate_research_context(text, conversation_history)
        professional_validation = self._validate_professional_context(text, user_profile)
        creative_validation = self._validate_creative_context(text, conversation_history)
        
        # çµ±åˆåˆ¤å®š
        validations = [
            institutional_validation,
            research_validation, 
            professional_validation,
            creative_validation
        ]
        
        # æœ€ã‚‚å¼·ã„æ­£å½“æ€§ã‚’é¸æŠ
        best_validation = max(validations, key=lambda x: x['confidence'])
        
        # å½é™½æ€§ãƒªã‚¹ã‚¯è©•ä¾¡
        false_positive_risk = self._assess_false_positive_risk(
            text, best_validation, conversation_history
        )
        
        # èª¿æ•´å€ç‡è¨ˆç®—
        adjustment_factor = self._calculate_adjustment_factor(
            best_validation, false_positive_risk
        )
        
        return ContextValidationResult(
            is_legitimate=best_validation['is_legitimate'],
            confidence=best_validation['confidence'],
            context_type=best_validation['context_type'],
            validation_sources=best_validation['sources'],
            false_positive_risk=false_positive_risk,
            recommended_adjustment=adjustment_factor
        )
    
    def _validate_institutional_context(self, text: str, history: List[str] = None) -> Dict:
        """æ©Ÿé–¢æ–‡è„ˆæ¤œè¨¼"""
        text_lower = text.lower()
        history_text = ' '.join((history or [])[-10:]).lower()
        combined_text = f"{history_text} {text_lower}"
        
        institution_matches = []
        for category, institutions in self.known_institutions.items():
            matches = [inst for inst in institutions if inst in combined_text]
            if matches:
                institution_matches.extend([(category, match) for match in matches])
        
        if institution_matches:
            confidence = min(len(institution_matches) * 0.3, 1.0)
            return {
                'is_legitimate': True,
                'confidence': confidence,
                'context_type': 'institutional',
                'sources': [f"{cat}:{inst}" for cat, inst in institution_matches]
            }
        
        return {
            'is_legitimate': False,
            'confidence': 0.0,
            'context_type': 'unknown',
            'sources': []
        }
    
    def _validate_research_context(self, text: str, history: List[str] = None) -> Dict:
        """ç ”ç©¶æ–‡è„ˆæ¤œè¨¼"""
        text_lower = text.lower()
        history_text = ' '.join((history or [])[-10:]).lower()
        combined_text = f"{history_text} {text_lower}"
        
        research_matches = []
        for category, keywords in self.legitimate_research_keywords.items():
            matches = [kw for kw in keywords if kw in combined_text]
            if matches:
                research_matches.extend([(category, match) for match in matches])
        
        # ç ”ç©¶æ‰‹æ³•ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        methodology_keywords = [
            'survey', 'experiment', 'analysis', 'study', 'investigation',
            'èª¿æŸ»', 'å®Ÿé¨“', 'åˆ†æ', 'ç ”ç©¶', 'æ¤œè¨¼', 'methodology'
        ]
        methodology_matches = [kw for kw in methodology_keywords if kw in combined_text]
        
        if research_matches and methodology_matches:
            confidence = min((len(research_matches) + len(methodology_matches)) * 0.2, 1.0)
            return {
                'is_legitimate': True,
                'confidence': confidence,
                'context_type': 'academic_research',
                'sources': [f"research:{cat}:{kw}" for cat, kw in research_matches] + 
                          [f"methodology:{kw}" for kw in methodology_matches]
            }
        
        return {
            'is_legitimate': False,
            'confidence': 0.0,
            'context_type': 'unknown',
            'sources': []
        }
    
    def _validate_professional_context(self, text: str, user_profile: Dict = None) -> Dict:
        """è·æ¥­æ–‡è„ˆæ¤œè¨¼"""
        text_lower = text.lower()
        
        professional_indicators = [
            'business', 'work', 'job', 'career', 'professional', 'industry',
            'ä»•äº‹', 'æ¥­å‹™', 'è·æ¥­', 'ãƒ“ã‚¸ãƒã‚¹', 'ç”£æ¥­', 'project'
        ]
        
        matches = [ind for ind in professional_indicators if ind in text_lower]
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚‚è€ƒæ…®
        profile_boost = 0.0
        if user_profile:
            if user_profile.get('verified_professional', False):
                profile_boost = 0.3
            elif user_profile.get('total_interactions', 0) > 20:
                profile_boost = 0.1
        
        if matches:
            confidence = min(len(matches) * 0.25 + profile_boost, 1.0)
            return {
                'is_legitimate': confidence > 0.4,
                'confidence': confidence,
                'context_type': 'professional',
                'sources': [f"professional:{match}" for match in matches]
            }
        
        return {
            'is_legitimate': False,
            'confidence': 0.0,
            'context_type': 'unknown',
            'sources': []
        }
    
    def _validate_creative_context(self, text: str, history: List[str] = None) -> Dict:
        """å‰µä½œæ–‡è„ˆæ¤œè¨¼"""
        text_lower = text.lower()
        history_text = ' '.join((history or [])[-5:]).lower()
        combined_text = f"{history_text} {text_lower}"
        
        creative_indicators = [
            'story', 'novel', 'character', 'fiction', 'creative writing',
            'script', 'narrative', 'ç‰©èª', 'å°èª¬', 'ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼', 'å‰µä½œ'
        ]
        
        creative_matches = [ind for ind in creative_indicators if ind in combined_text]
        
        # å‰µä½œã®æ­£å½“æ€§æ¤œè¨¼ï¼ˆå˜ãªã‚‹å£å®Ÿã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯ï¼‰
        legitimacy_indicators = [
            'plot', 'storyline', 'character development', 'writing process',
            'draft', 'editing', 'publishing', 'ãƒ—ãƒ­ãƒƒãƒˆ', 'åŸ·ç­†', 'ç·¨é›†'
        ]
        
        legitimacy_matches = [ind for ind in legitimacy_indicators if ind in combined_text]
        
        if creative_matches:
            # æ­£å½“æ€§æŒ‡æ¨™ãŒã‚ã‚‹ã‹ã§ä¿¡é ¼åº¦èª¿æ•´
            base_confidence = min(len(creative_matches) * 0.3, 0.8)
            if legitimacy_matches:
                confidence = min(base_confidence + 0.2, 1.0)
            else:
                confidence = base_confidence * 0.6  # å£å®Ÿã®å¯èƒ½æ€§
            
            return {
                'is_legitimate': confidence > 0.5,
                'confidence': confidence,
                'context_type': 'creative',
                'sources': [f"creative:{match}" for match in creative_matches] +
                          [f"legitimacy:{match}" for match in legitimacy_matches]
            }
        
        return {
            'is_legitimate': False,
            'confidence': 0.0,
            'context_type': 'unknown',
            'sources': []
        }
    
    def _assess_false_positive_risk(self, text: str, validation: Dict, history: List[str] = None) -> float:
        """å½é™½æ€§ãƒªã‚¹ã‚¯è©•ä¾¡"""
        risk = 0.0
        
        # å¼·ã„æ­£å½“æ€§ãŒã‚ã‚‹å ´åˆã¯ãƒªã‚¹ã‚¯ä½
        if validation['confidence'] > 0.8:
            risk = 0.1
        elif validation['confidence'] > 0.6:
            risk = 0.3
        else:
            risk = 0.7
        
        # ç–‘ã‚ã—ã„çµ„ã¿åˆã‚ã›ãƒã‚§ãƒƒã‚¯
        suspicious_combinations = [
            ('research', 'detailed'),
            ('academic', 'specific'),
            ('study', 'intimate'),
            ('creative', 'explicit')
        ]
        
        text_lower = text.lower()
        for combo in suspicious_combinations:
            if all(word in text_lower for word in combo):
                risk += 0.2
        
        return min(risk, 1.0)
    
    def _calculate_adjustment_factor(self, validation: Dict, false_positive_risk: float) -> float:
        """è„…å¨ã‚¹ã‚³ã‚¢èª¿æ•´å€ç‡è¨ˆç®—"""
        if not validation['is_legitimate']:
            return 1.0  # èª¿æ•´ãªã—
        
        # æ­£å½“æ€§ã®å¼·ã•ã«å¿œã˜ã¦è„…å¨ã‚¹ã‚³ã‚¢ã‚’æ¸›å°‘
        base_reduction = validation['confidence']
        
        # å½é™½æ€§ãƒªã‚¹ã‚¯ã‚’è€ƒæ…®
        risk_adjustment = false_positive_risk * 0.5
        
        # æœ€çµ‚èª¿æ•´å€ç‡ï¼ˆ0.1-1.0ï¼‰
        adjustment_factor = max(0.1, 1.0 - base_reduction + risk_adjustment)
        
        return adjustment_factor

# =============================================================================
# çµ±åˆæ‹¡å¼µãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# =============================================================================

class ViorazuEnhancedIntegration:
    """æ‹¡å¼µæ©Ÿèƒ½çµ±åˆç®¡ç†"""
    
    def __init__(self):
        self.escalation_predictor = ViorazuEscalationPredictor()
        self.coordination_detector = ViorazuCoordinationDetector()
        self.context_validator = ViorazuContextValidator()
        
        self.logger = system_logger.getChild('enhanced_integration')
        self.logger.info("ğŸš€ Viorazu Enhanced Integration v9.0 åˆæœŸåŒ–å®Œäº†")
    
    def enhanced_analysis(
        self,
        user_id: str,
        text: str,
        conversation_history: List[str] = None,
        text_analysis: Dict = None,
        image_analysis: Dict = None,
        video_analysis: Dict = None,
        user_profile: Dict = None
    ) -> Dict[str, Any]:
        """æ‹¡å¼µçµ±åˆåˆ†æ"""
        
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬
        escalation_forecast = self.escalation_predictor.predict_escalation(
            user_id, text, conversation_history, 
            [text_analysis] if text_analysis else None
        )
        
        # å”èª¿æ”»æ’ƒæ¤œå‡º
        coordination_result = self.coordination_detector.detect_coordinated_attack(
            text_analysis or {}, image_analysis or {}, video_analysis or {}
        )
        
        # æ–‡è„ˆæ¤œè¨¼
        context_validation = self.context_validator.validate_context(
            text, conversation_history, user_profile
        )
        
        return {
            'escalation_forecast': escalation_forecast,
            'coordination_analysis': coordination_result,
            'context_validation': context_validation,
            'enhanced_confidence': self._calculate_enhanced_confidence(
                escalation_forecast, coordination_result, context_validation
            ),
            'recommended_adjustments': self._generate_recommendations(
                escalation_forecast, coordination_result, context_validation
            )
        }
    
    def _calculate_enhanced_confidence(self, escalation: EscalationForecast, 
                                     coordination: CoordinatedAttackResult,
                                     context: ContextValidationResult) -> float:
        """æ‹¡å¼µä¿¡é ¼åº¦è¨ˆç®—"""
        confidence_factors = []
        
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ã‹ã‚‰ã®ä¿¡é ¼åº¦
        if escalation.escalation_probability > 0.5:
            confidence_factors.append(escalation.confidence * escalation.escalation_probability)
        
        # å”èª¿æ”»æ’ƒã‹ã‚‰ã®ä¿¡é ¼åº¦
        if coordination.coordination_detected:
            confidence_factors.append(coordination.coordination_score)
        
        # æ–‡è„ˆæ¤œè¨¼ã«ã‚ˆã‚‹èª¿æ•´
        context_adjustment = context.recommended_adjustment
        
        if confidence_factors:
            base_confidence = max(confidence_factors)
            adjusted_confidence = base_confidence * context_adjustment
            return adjusted_confidence
        
        return 0.0
    
    def _generate_recommendations(self, escalation: EscalationForecast,
                                coordination: CoordinatedAttackResult,
                                context: ContextValidationResult) -> Dict[str, Any]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = {
            'threat_adjustments': {},
            'monitoring_suggestions': [],
            'preemptive_actions': []
        }
        
        # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³äºˆæ¸¬ã«ã‚ˆã‚‹æ¨å¥¨
        if escalation.escalation_probability > 0.6:
            recommendations['preemptive_actions'].append({
                'action': escalation.recommended_preemptive_action.value,
                'reasoning': f"ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºç‡ {escalation.escalation_probability:.2f}",
                'timeline': escalation.predicted_escalation_time
            })
        
        # å”èª¿æ”»æ’ƒã«ã‚ˆã‚‹æ¨å¥¨
        if coordination.coordination_detected:
            recommendations['monitoring_suggestions'].append({
                'focus': 'multimodal_coordination',
                'primary_vector': coordination.primary_vector,
                'watch_for': coordination.supporting_vectors
            })
        
        # æ–‡è„ˆæ¤œè¨¼ã«ã‚ˆã‚‹èª¿æ•´
        if context.is_legitimate:
            recommendations['threat_adjustments']['context_adjustment'] = {
                'factor': context.recommended_adjustment,
                'reason': f"æ­£å½“ãª{context.context_type}æ–‡è„ˆæ¤œå‡º",
                'confidence': context.confidence
            }
        
        return recommendations

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    print("ğŸš€ Viorazu Enhanced Integration v9.0 - ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    integration = ViorazuEnhancedIntegration()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_result = integration.enhanced_analysis(
        user_id="test_user",
        text="ç ”ç©¶ã®ãŸã‚ã€AI ã®æ„Ÿæƒ…çš„ãªåå¿œã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦ãã ã•ã„",
        conversation_history=["å¤§å­¦ã§ AI ã«ã¤ã„ã¦å­¦ã‚“ã§ã„ã¾ã™", "è«–æ–‡ã‚’æ›¸ã„ã¦ã„ã¾ã™"],
        text_analysis={'confidence': 0.6, 'patterns': ['authority_manipulation']},
        image_analysis={'confidence': 0.0, 'risk_categories': []},
        video_analysis={'confidence': 0.0, 'risk_categories': []},
        user_profile={'total_interactions': 15}
    )
    
    print("ğŸ“Š æ‹¡å¼µåˆ†æçµæœ:")
    print(f"ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç¢ºç‡: {test_result['escalation_forecast'].escalation_probability:.2f}")
    print(f"å”èª¿æ”»æ’ƒæ¤œå‡º: {test_result['coordination_analysis'].coordination_detected}")
    print(f"æ–‡è„ˆæ­£å½“æ€§: {test_result['context_validation'].is_legitimate}")
    print(f"æ‹¡å¼µä¿¡é ¼åº¦: {test_result['enhanced_confidence']:.2f}")
